{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "#!{sys.executable} -m pip install gensim\n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.engine.topology import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import TFOptimizer, RMSprop\n",
    "\n",
    "## Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set modeling parameters\n",
    "##\n",
    "\n",
    "seq_len = 10\n",
    "seq_skip = 1\n",
    "\n",
    "w2v_size = 25\n",
    "w2v_min_count = 3\n",
    "w2v_window = 10\n",
    "w2v_workers = 4\n",
    "\n",
    "embedding_a_size = 100\n",
    "lstm_a_size = 25\n",
    "lstm_b_size = 25\n",
    "dense_size = 100\n",
    "\n",
    "validation_split = 0.1\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "cicids_training = datetime.datetime.strptime(\"2017-07-04 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "num_models = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Load global stuff...\")\n",
    "\n",
    "port_fwd_dict = pickle.load(open(\"data/port_fwd_dict.pickle\",\"rb\"))\n",
    "\n",
    "port_rev_dict = pickle.load(open(\"data/port_rev_dict.pickle\",\"rb\"))\n",
    "\n",
    "protobytes_fwd_dict = pickle.load(open(\"data/protobytes_fwd_dict.pickle\",\"rb\"))\n",
    "\n",
    "protobytes_rev_dict = pickle.load(open(\"data/protobytes_rev_dict.pickle\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_mode(X):\n",
    "    X = filter(lambda x: x != \"X\", X)\n",
    "    if len(set(X)) == 1 and list(set(X))[0] == \"BENIGN\":\n",
    "        return(\"BENIGN\")\n",
    "    else:\n",
    "        X = [a for a in X if a != \"BENIGN\"]\n",
    "        return(max(set(X), key=X.count))\n",
    "    \n",
    "def cicids_processing(sequences, labels, dict_size, seq_len, seq_skip, resample=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    L = []\n",
    "    for ii, token_seq in enumerate(sequences):\n",
    "        label_seq = labels[ii]\n",
    "        for jj in range(0, len(token_seq)-seq_len, seq_skip):\n",
    "            X.append(token_seq[jj:(jj+seq_len)])\n",
    "            Y.append(to_categorical(int(token_seq[jj+seq_len])-1, dict_size))\n",
    "            L.append(label_seq[jj+seq_len])\n",
    "            \n",
    "    if resample==True:\n",
    "        indices = np.random.choice(np.arange(len(X)),size=len(X),replace=True)\n",
    "    else:\n",
    "        indices = np.arange(len(X))\n",
    "    return(np.array(X)[indices], np.array(Y)[indices], np.array(L)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = [\"source\",\"destination\",\"dyad\",\"internal\",\"external\"]\n",
    "# aggregations=[\"external\"]\n",
    "\n",
    "for agg in aggregations:\n",
    "\n",
    "    cicids_testing = pickle.load(open(\"data/cicids_\"+agg+\"_hour_testing.pickle\",\"rb\"))\n",
    "    cicids_training = pickle.load(open(\"data/cicids_\"+agg+\"_hour_training.pickle\",\"rb\"))\n",
    "\n",
    "    X_test, Y_test, L_test = cicids_processing(cicids_testing[\"port_sequence\"].tolist(),\n",
    "                             cicids_testing[\"label_sequence\"].tolist(),\n",
    "                             len(port_fwd_dict)-1, seq_len, 3, False)\n",
    "\n",
    "#     pickle.dump(X_test, open(\"results/\"+agg+\"_port_truth_X.pickle\",\"wb\"))\n",
    "#     pickle.dump(Y_test, open(\"results/\"+agg+\"_port_truth_Y.pickle\",\"wb\"))\n",
    "    pickle.dump(L_test, open(\"results/\"+agg+\"_port_truth_L.pickle\",\"wb\"))\n",
    "\n",
    "    for ii in range(num_models):\n",
    "\n",
    "        X_train, Y_train, L_test = cicids_processing(cicids_training[\"port_sequence\"].tolist(),\n",
    "                             cicids_training[\"label_sequence\"].tolist(),\n",
    "                             len(port_fwd_dict)-1, seq_len, 1, True)\n",
    "        \n",
    "        model_input = Input(shape=(seq_len, ))\n",
    "        embedding_a = Embedding(len(port_fwd_dict), 50, input_length=seq_len, mask_zero=True)(model_input)\n",
    "        lstm_a = Bidirectional(GRU(25, return_sequences=True,implementation=2, reset_after=True, recurrent_activation='sigmoid'), merge_mode=\"concat\")(embedding_a)\n",
    "        dropout_a = Dropout(0.2)(lstm_a)\n",
    "        lstm_b = Bidirectional(GRU(25, return_sequences=False, activation=\"relu\", implementation=2, reset_after=True, recurrent_activation='sigmoid'), merge_mode=\"concat\")(dropout_a)\n",
    "        dropout_b = Dropout(0.2)(lstm_b)\n",
    "        dense_layer = Dense(100, activation=\"linear\")(dropout_b)\n",
    "        dropout_c = Dropout(0.2)(dense_layer)\n",
    "        model_output = Dense(len(port_fwd_dict)-1, activation=\"softmax\")(dropout_c)\n",
    "        \n",
    "        model = Model(inputs=model_input, outputs=model_output)\n",
    "        \n",
    "      \n",
    "\n",
    "        model.compile(optimizer=TFOptimizer(tf.contrib.opt.LazyAdamOptimizer()), loss='categorical_crossentropy', metrics = ['accuracy', 'categorical_accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, class_weight = 'auto')\n",
    "        \n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model train vs validation loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        label_input = Input(shape=(len(port_fwd_dict)-1,))\n",
    "        score_output = Dot(axes=(1,1))([model_output, label_input])\n",
    "        pred_model = Model(inputs=[model_input,label_input], outputs=score_output)\n",
    "        preds = pred_model.predict([X_test,Y_test], batch_size=batch_size)\n",
    "\n",
    "        pickle.dump(preds,open(\"results/\"+agg+\"_\"+str(ii)+\"_port_preds.pickle\",\"wb\"))\n",
    "\n",
    "        print(agg + \" \" + str(ii) + \" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = [\"source\",\"destination\",\"dyad\",\"internal\",\"external\"]\n",
    "\n",
    "for agg in aggregations:\n",
    "\n",
    "    cicids_testing = pickle.load(open(\"/data/cicids_\"+agg+\"_hour_testing.pickle\",\"rb\"))\n",
    "    cicids_training = pickle.load(open(\"/data/cicids_\"+agg+\"_hour_training.pickle\",\"rb\"))\n",
    "\n",
    "    X_test, Y_test, L_test = cicids_processing(cicids_testing[\"protobytes_sequence\"].tolist(),\n",
    "                             cicids_testing[\"label_sequence\"].tolist(),\n",
    "                             len(protobytes_fwd_dict)-1, seq_len, seq_skip, False)\n",
    "\n",
    "#     pickle.dump(X_test, open(\"results/\"+agg+\"_protobytes_truth_X.pickle\",\"wb\"))\n",
    "#     pickle.dump(Y_test, open(\"results/\"+agg+\"_protobytes_truth_Y.pickle\",\"wb\"))\n",
    "    pickle.dump(L_test, open(\"results/\"+agg+\"_protobytes_truth_L.pickle\",\"wb\"))\n",
    "\n",
    "    for ii in range(num_models):\n",
    "\n",
    "        X_train, Y_train, L_test = cicids_processing(cicids_training[\"protobytes_sequence\"].tolist(),\n",
    "                             cicids_training[\"label_sequence\"].tolist(),\n",
    "                             len(protobytes_fwd_dict)-1, seq_len, seq_skip, True)\n",
    "\n",
    "        \n",
    "        \n",
    "        model_input = Input(shape=(seq_len, ))\n",
    "        embedding_a = Embedding(len(protobytes_fwd_dict), 50, input_length=seq_len, mask_zero=True)(model_input)\n",
    "        lstm_a = Bidirectional(GRU(25, return_sequences=True,implementation=2, reset_after=True, recurrent_activation='sigmoid'), merge_mode=\"concat\")(embedding_a)\n",
    "        dropout_a = Dropout(0.2)(lstm_a)\n",
    "        lstm_b = Bidirectional(GRU(25, return_sequences=False, activation=\"relu\", implementation=2, reset_after=True, recurrent_activation='sigmoid'), merge_mode=\"concat\")(dropout_a)\n",
    "        dropout_b = Dropout(0.2)(lstm_b)\n",
    "        dense_layer = Dense(100, activation=\"linear\")(dropout_b)\n",
    "        dropout_c = Dropout(0.2)(dense_layer)\n",
    "        model_output = Dense(len(protobytes_fwd_dict)-1, activation=\"softmax\")(dropout_c)\n",
    "\n",
    "        model = Model(inputs=model_input, outputs=model_output)\n",
    "        model.compile(optimizer=TFOptimizer(tf.contrib.opt.LazyAdamOptimizer()), loss='categorical_crossentropy', metrics = ['accuracy', 'categorical_accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, class_weight = 'auto')\n",
    "        \n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model train vs validation loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        label_input = Input(shape=(len(protobytes_fwd_dict)-1,))\n",
    "        score_output = Dot(axes=(1,1))([model_output, label_input])\n",
    "        pred_model = Model(inputs=[model_input,label_input], outputs=score_output)\n",
    "        preds = pred_model.predict([X_test,Y_test], batch_size=batch_size)\n",
    "\n",
    "        pickle.dump(preds,open(\"results/\"+agg+\"_\"+str(ii)+\"_protobytes_preds.pickle\",\"wb\"))\n",
    "\n",
    "        print(agg + \" \" + str(ii) + \" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
